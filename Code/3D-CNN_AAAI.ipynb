{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da505d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, TensorDataset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Custom Video Dataset\n",
    "class CustomVideoDataset(Dataset):\n",
    "    def __init__(self, video_folder, num_frames, frame_height, frame_width):\n",
    "        self.video_folder = video_folder\n",
    "        self.video_files = []\n",
    "\n",
    "        for folder in video_folder:\n",
    "            folder_path = os.path.join(folder)\n",
    "            self.video_files.extend([os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.mp4')])\n",
    "        self.num_frames = num_frames\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_files[idx]\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Explicitly convert to grayscale\n",
    "            frame = cv2.resize(frame, (self.frame_width, self.frame_height))  # Resizing the frame\n",
    "            frames.append(frame)\n",
    "            if len(frames) == self.num_frames:\n",
    "                break\n",
    "        cap.release()\n",
    "\n",
    "        # Handle case where video is shorter than num_frames\n",
    "        while len(frames) < self.num_frames:\n",
    "            frames.append(np.zeros((self.frame_height, self.frame_width), dtype=np.float32))  # Gray frame\n",
    "\n",
    "        video_tensor = torch.tensor(np.stack(frames, axis=0)).unsqueeze(1).float() / 255  # Convert to torch tensor and normalize to [0,1]\n",
    "        video_tensor = video_tensor.permute(1, 0, 2, 3)  # Reorder dimensions to [1, 16, 224, 224]\n",
    "\n",
    "        video_action = \"_\".join(video_path.split(\"/\")[-2].split('_')[:2])\n",
    "        label = 0\n",
    "        action_to_label = {\n",
    "            'Arm_Swing': 0, 'Body_Swing': 1, 'Chest_Expansion': 2, 'Drumming_Pose': 3,\n",
    "            'Frog_Pose': 4, 'Marcas_Forward': 5, 'Marcas_Shaking': 6,\n",
    "            'Squat_Pose': 7, 'Twist_Pose': 8, 'Sing_Clap': 9, 'Tree_Pose': 10\n",
    "        }\n",
    "        \n",
    "    \n",
    "        label = action_to_label.get(video_action, label)\n",
    "\n",
    "        # Convert label to tensor\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return video_tensor, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45765884",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm3d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# ResNet3D Model\n",
    "class ResNet3D(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=11):\n",
    "        super(ResNet3D, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv3d(1, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm3d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self.make_layer(block, 64, layers[0], stride=1)\n",
    "        self.layer2 = self.make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self.make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self.make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def make_layer(self, block, out_channels, blocks, stride):\n",
    "        strides = [stride] + [1] * (blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        action_output = self.fc(x)\n",
    "        return action_output\n",
    "\n",
    "def resnet3d():\n",
    "    return ResNet3D(ResidualBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f2d8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_folder = [\n",
    "    \n",
    "   \n",
    "    'Folder_Data',\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "num_frames = 40\n",
    "frame_height = 200\n",
    "frame_width = 200\n",
    "\n",
    "dataset = CustomVideoDataset(video_folder=video_folder, num_frames=num_frames, frame_height=frame_height, frame_width=frame_width)\n",
    "\n",
    "validation_split = 0.2\n",
    "shuffle_dataset = True\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "if shuffle_dataset:\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f49ca8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = resnet3d().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2000bce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b8b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100], Train Loss: 1.3811, Validation Loss: 1.5575, Validation Accuracy: 46.95%\n",
      "Epoch [1/100], Train Loss: 0.8683, Validation Loss: 1.0766, Validation Accuracy: 62.04%\n",
      "Epoch [2/100], Train Loss: 0.7267, Validation Loss: 1.1294, Validation Accuracy: 64.18%\n",
      "Epoch [3/100], Train Loss: 0.6260, Validation Loss: 0.8677, Validation Accuracy: 69.97%\n",
      "Epoch [4/100], Train Loss: 0.5595, Validation Loss: 0.7790, Validation Accuracy: 72.41%\n",
      "Epoch [5/100], Train Loss: 0.4913, Validation Loss: 1.1980, Validation Accuracy: 62.20%\n",
      "Epoch [6/100], Train Loss: 0.4064, Validation Loss: 3.0480, Validation Accuracy: 39.63%\n",
      "Epoch [7/100], Train Loss: 0.3035, Validation Loss: 0.6645, Validation Accuracy: 79.57%\n",
      "Epoch [8/100], Train Loss: 0.2296, Validation Loss: 0.9028, Validation Accuracy: 73.17%\n",
      "Epoch [9/100], Train Loss: 0.2115, Validation Loss: 2.0770, Validation Accuracy: 63.26%\n",
      "Epoch [10/100], Train Loss: 0.1648, Validation Loss: 0.7064, Validation Accuracy: 77.13%\n",
      "Epoch [11/100], Train Loss: 0.1122, Validation Loss: 0.7563, Validation Accuracy: 78.51%\n",
      "Epoch [12/100], Train Loss: 0.1216, Validation Loss: 0.7837, Validation Accuracy: 77.90%\n",
      "Epoch [13/100], Train Loss: 0.0486, Validation Loss: 0.8320, Validation Accuracy: 77.74%\n",
      "Epoch [14/100], Train Loss: 0.0442, Validation Loss: 1.5206, Validation Accuracy: 68.14%\n",
      "Epoch [15/100], Train Loss: 0.0935, Validation Loss: 1.1303, Validation Accuracy: 71.49%\n",
      "Epoch [16/100], Train Loss: 0.1149, Validation Loss: 1.2318, Validation Accuracy: 71.49%\n",
      "Epoch [17/100], Train Loss: 0.0405, Validation Loss: 0.7707, Validation Accuracy: 80.49%\n",
      "Epoch [18/100], Train Loss: 0.0350, Validation Loss: 0.8751, Validation Accuracy: 76.83%\n",
      "Epoch [19/100], Train Loss: 0.0266, Validation Loss: 0.7454, Validation Accuracy: 80.34%\n",
      "Epoch [20/100], Train Loss: 0.0217, Validation Loss: 0.7707, Validation Accuracy: 78.66%\n",
      "Epoch [21/100], Train Loss: 0.0861, Validation Loss: 0.8462, Validation Accuracy: 77.59%\n",
      "Epoch [22/100], Train Loss: 0.0331, Validation Loss: 1.1902, Validation Accuracy: 72.71%\n",
      "Epoch [23/100], Train Loss: 0.0418, Validation Loss: 1.8085, Validation Accuracy: 64.79%\n",
      "Epoch [24/100], Train Loss: 0.0513, Validation Loss: 0.9703, Validation Accuracy: 76.37%\n",
      "Epoch [25/100], Train Loss: 0.0539, Validation Loss: 0.8259, Validation Accuracy: 80.49%\n",
      "Epoch [26/100], Train Loss: 0.0152, Validation Loss: 0.8522, Validation Accuracy: 77.90%\n",
      "Epoch [27/100], Train Loss: 0.0085, Validation Loss: 0.7222, Validation Accuracy: 82.01%\n",
      "Epoch [28/100], Train Loss: 0.0623, Validation Loss: 0.9794, Validation Accuracy: 76.52%\n",
      "Epoch [29/100], Train Loss: 0.0385, Validation Loss: 0.8611, Validation Accuracy: 78.81%\n",
      "Epoch [30/100], Train Loss: 0.0344, Validation Loss: 1.0315, Validation Accuracy: 77.13%\n",
      "Epoch [31/100], Train Loss: 0.0131, Validation Loss: 0.7711, Validation Accuracy: 80.79%\n",
      "Epoch [32/100], Train Loss: 0.0044, Validation Loss: 0.7408, Validation Accuracy: 81.10%\n",
      "Epoch [33/100], Train Loss: 0.0031, Validation Loss: 0.7363, Validation Accuracy: 82.47%\n",
      "Epoch [34/100], Train Loss: 0.0026, Validation Loss: 0.9518, Validation Accuracy: 78.66%\n",
      "Epoch [35/100], Train Loss: 0.0215, Validation Loss: 0.9890, Validation Accuracy: 77.44%\n",
      "Epoch [36/100], Train Loss: 0.0490, Validation Loss: 3.0325, Validation Accuracy: 50.61%\n",
      "Epoch [37/100], Train Loss: 0.0755, Validation Loss: 0.9543, Validation Accuracy: 76.83%\n",
      "Epoch [38/100], Train Loss: 0.0679, Validation Loss: 1.2674, Validation Accuracy: 71.34%\n",
      "Epoch [39/100], Train Loss: 0.0596, Validation Loss: 0.8345, Validation Accuracy: 79.12%\n",
      "Epoch [40/100], Train Loss: 0.0151, Validation Loss: 0.7938, Validation Accuracy: 80.79%\n",
      "Epoch [41/100], Train Loss: 0.0043, Validation Loss: 0.7496, Validation Accuracy: 81.40%\n",
      "Epoch [42/100], Train Loss: 0.0056, Validation Loss: 0.7693, Validation Accuracy: 80.18%\n",
      "Epoch [43/100], Train Loss: 0.0054, Validation Loss: 0.8344, Validation Accuracy: 81.10%\n",
      "Epoch [44/100], Train Loss: 0.0687, Validation Loss: 1.0895, Validation Accuracy: 75.61%\n",
      "Epoch [45/100], Train Loss: 0.0181, Validation Loss: 0.7742, Validation Accuracy: 80.34%\n",
      "Epoch [46/100], Train Loss: 0.0171, Validation Loss: 0.8399, Validation Accuracy: 79.27%\n",
      "Epoch [47/100], Train Loss: 0.0103, Validation Loss: 0.7646, Validation Accuracy: 80.79%\n",
      "Epoch [48/100], Train Loss: 0.0239, Validation Loss: 0.8310, Validation Accuracy: 80.49%\n",
      "Epoch [53/100], Train Loss: 0.0021, Validation Loss: 1.0058, Validation Accuracy: 78.05%\n",
      "Epoch [54/100], Train Loss: 0.0050, Validation Loss: 0.8206, Validation Accuracy: 80.95%\n",
      "Epoch [55/100], Train Loss: 0.0282, Validation Loss: 0.9884, Validation Accuracy: 77.59%\n",
      "Epoch [56/100], Train Loss: 0.0247, Validation Loss: 0.9650, Validation Accuracy: 76.68%\n",
      "Epoch [57/100], Train Loss: 0.0290, Validation Loss: 1.0117, Validation Accuracy: 79.12%\n",
      "Epoch [58/100], Train Loss: 0.0119, Validation Loss: 0.9314, Validation Accuracy: 78.66%\n",
      "Epoch [59/100], Train Loss: 0.0093, Validation Loss: 0.9550, Validation Accuracy: 80.49%\n",
      "Epoch [60/100], Train Loss: 0.0621, Validation Loss: 1.1297, Validation Accuracy: 74.70%\n",
      "Epoch [61/100], Train Loss: 0.0293, Validation Loss: 0.8681, Validation Accuracy: 79.27%\n",
      "Epoch [62/100], Train Loss: 0.0128, Validation Loss: 0.9611, Validation Accuracy: 75.46%\n",
      "Epoch [63/100], Train Loss: 0.0116, Validation Loss: 0.8951, Validation Accuracy: 81.25%\n",
      "Epoch [64/100], Train Loss: 0.0064, Validation Loss: 0.8261, Validation Accuracy: 80.95%\n",
      "Epoch [65/100], Train Loss: 0.0095, Validation Loss: 0.8775, Validation Accuracy: 79.42%\n",
      "Epoch [66/100], Train Loss: 0.0522, Validation Loss: 0.8992, Validation Accuracy: 80.95%\n",
      "Epoch [67/100], Train Loss: 0.0172, Validation Loss: 0.8510, Validation Accuracy: 80.03%\n",
      "Epoch [68/100], Train Loss: 0.0149, Validation Loss: 0.8468, Validation Accuracy: 82.01%\n",
      "Epoch [69/100], Train Loss: 0.0080, Validation Loss: 0.8486, Validation Accuracy: 80.49%\n",
      "Epoch [70/100], Train Loss: 0.0063, Validation Loss: 0.8535, Validation Accuracy: 80.95%\n",
      "Epoch [71/100], Train Loss: 0.0020, Validation Loss: 0.8929, Validation Accuracy: 81.25%\n",
      "Epoch [72/100], Train Loss: 0.0136, Validation Loss: 0.9525, Validation Accuracy: 80.03%\n",
      "Epoch [73/100], Train Loss: 0.0141, Validation Loss: 1.0884, Validation Accuracy: 77.13%\n",
      "Epoch [74/100], Train Loss: 0.0178, Validation Loss: 1.0336, Validation Accuracy: 79.27%\n",
      "Epoch [75/100], Train Loss: 0.0306, Validation Loss: 0.8661, Validation Accuracy: 80.49%\n",
      "Epoch [76/100], Train Loss: 0.0046, Validation Loss: 0.8763, Validation Accuracy: 81.25%\n",
      "Epoch [77/100], Train Loss: 0.0024, Validation Loss: 0.9585, Validation Accuracy: 82.62%\n",
      "Epoch [78/100], Train Loss: 0.0082, Validation Loss: 1.0134, Validation Accuracy: 79.73%\n",
      "Epoch [79/100], Train Loss: 0.0386, Validation Loss: 1.0846, Validation Accuracy: 78.35%\n",
      "Epoch [80/100], Train Loss: 0.0169, Validation Loss: 0.9224, Validation Accuracy: 82.01%\n",
      "Epoch [81/100], Train Loss: 0.0093, Validation Loss: 0.9662, Validation Accuracy: 81.25%\n",
      "Epoch [82/100], Train Loss: 0.0090, Validation Loss: 0.9679, Validation Accuracy: 80.79%\n",
      "Epoch [83/100], Train Loss: 0.0092, Validation Loss: 0.9525, Validation Accuracy: 80.49%\n",
      "Epoch [84/100], Train Loss: 0.0289, Validation Loss: 0.9341, Validation Accuracy: 80.64%\n",
      "Epoch [85/100], Train Loss: 0.0052, Validation Loss: 0.9305, Validation Accuracy: 81.71%\n",
      "Epoch [86/100], Train Loss: 0.0013, Validation Loss: 0.8890, Validation Accuracy: 82.16%\n",
      "Epoch [87/100], Train Loss: 0.0010, Validation Loss: 0.8990, Validation Accuracy: 82.32%\n",
      "Epoch [88/100], Train Loss: 0.0006, Validation Loss: 0.9122, Validation Accuracy: 81.40%\n",
      "Epoch [89/100], Train Loss: 0.0006, Validation Loss: 0.9046, Validation Accuracy: 81.86%\n",
      "Epoch [90/100], Train Loss: 0.0195, Validation Loss: 1.3446, Validation Accuracy: 77.13%\n",
      "Epoch [91/100], Train Loss: 0.0818, Validation Loss: 1.0866, Validation Accuracy: 76.37%\n",
      "Epoch [92/100], Train Loss: 0.0188, Validation Loss: 0.9374, Validation Accuracy: 80.03%\n",
      "Epoch [93/100], Train Loss: 0.0050, Validation Loss: 1.0564, Validation Accuracy: 79.42%\n",
      "Epoch [94/100], Train Loss: 0.0090, Validation Loss: 1.0792, Validation Accuracy: 78.51%\n",
      "Epoch [95/100], Train Loss: 0.0071, Validation Loss: 0.9185, Validation Accuracy: 80.03%\n",
      "Epoch [96/100], Train Loss: 0.0016, Validation Loss: 0.8628, Validation Accuracy: 82.16%\n",
      "Epoch [97/100], Train Loss: 0.0006, Validation Loss: 0.8413, Validation Accuracy: 82.32%\n",
      "Epoch [98/100], Train Loss: 0.0011, Validation Loss: 0.8845, Validation Accuracy: 81.40%\n",
      "Epoch [99/100], Train Loss: 0.0004, Validation Loss: 0.8544, Validation Accuracy: 82.47%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for videos, labels in train_loader:\n",
    "        videos, labels = videos.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(videos)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for videos, labels in val_loader:\n",
    "            videos, labels = videos.to(device), labels.to(device)\n",
    "            outputs = model(videos)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(model.state_dict(), 'CNN_Weights/' + f'model_epoch_{epoch}_new.pth')\n",
    "\n",
    "    \n",
    "\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}], Train Loss: {train_loss:.4f}, \"\n",
    "          f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9517de03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import f1_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a59606a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final F1 Score (weighted): 0.8245\n",
      "Class-wise F1-scores:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Arm_Swing       0.89      0.98      0.93        43\n",
      "     Body_Swing       0.83      0.78      0.80        50\n",
      "Chest_Expansion       0.92      0.86      0.89        56\n",
      "  Drumming_Pose       0.71      0.84      0.77        80\n",
      "      Frog_Pose       0.91      0.88      0.89        56\n",
      " Marcas_Forward       0.80      0.66      0.72        73\n",
      " Marcas_Shaking       0.74      0.80      0.77        70\n",
      "     Squat_Pose       0.78      0.91      0.84        34\n",
      "     Twist_Pose       0.88      0.92      0.90        64\n",
      "      Sing_Clap       0.73      0.70      0.72        63\n",
      "      Tree_Pose       0.98      0.87      0.92        67\n",
      "\n",
      "       accuracy                           0.82       656\n",
      "      macro avg       0.83      0.83      0.83       656\n",
      "   weighted avg       0.83      0.82      0.82       656\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate on the validation set\n",
    "model.eval()\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for videos, labels in val_loader:\n",
    "        videos, labels = videos.to(device), labels.to(device)\n",
    "        outputs = model(videos)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')  # Overall F1-score (weighted)\n",
    "class_report = classification_report(all_labels, all_preds, target_names=[\n",
    "    'Arm_Swing', 'Body_Swing', 'Chest_Expansion', 'Drumming_Pose', 'Frog_Pose', \n",
    "    'Marcas_Forward', 'Marcas_Shaking', 'Squat_Pose', 'Twist_Pose', 'Sing_Clap', 'Tree_Pose'\n",
    "])\n",
    "\n",
    "print(f\"Final F1 Score (weighted): {f1:.4f}\")\n",
    "print(\"Class-wise F1-scores:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e73ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
