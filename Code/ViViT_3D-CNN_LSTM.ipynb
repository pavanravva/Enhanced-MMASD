{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bed5c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.0.8)\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from timm) (2.2.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from timm) (0.17.0)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from timm) (0.24.5)\n",
      "Requirement already satisfied: safetensors in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from timm) (0.4.3)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub->timm) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub->timm) (2024.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub->timm) (21.3)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub->timm) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub->timm) (4.12.1)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->timm) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->timm) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->timm) (3.1.4)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torchvision->timm) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torchvision->timm) (10.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub->timm) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch->timm) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: tensorboardX in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.6.2.2)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboardX) (1.26.4)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboardX) (21.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboardX) (4.25.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging->tensorboardX) (3.1.2)\n",
      "Requirement already satisfied: einops in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install timm\n",
    "!pip install tensorboardX\n",
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ca727f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, TensorDataset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from timm.models.layers import drop_path, to_2tuple, trunc_normal_\n",
    "from timm.models.registry import register_model\n",
    "import torch.utils.checkpoint as checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a96bb2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVideoDataset(Dataset):\n",
    "    def __init__(self, video_folders_list, num_frames, frame_height, frame_width, frame_height_vivit, frame_width_vivit, num_frames_vivit):\n",
    "\n",
    "        self.video_files_1 =  video_folders_list\n",
    "        self.num_frames = num_frames\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "        self.frame_height_vivit = frame_height_vivit\n",
    "        self.frame_width_vivit = frame_width_vivit\n",
    "        self.num_frames_vivit = num_frames_vivit\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_files_1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        optical_flow_video = 'Complete_Files_Optical_Together' +  '/' + self.video_files_1[idx] + '.mp4'\n",
    "        romp_videos = 'Complete_Romp_Video_All_20_Frames_For_ViVit' + '/' + self.video_files_1[idx] + '.mp4'\n",
    "        skelton_data = 'Complete_3D_Skeleton_Data_Star' + '/' + self.video_files_1[idx] + '.csv'\n",
    "        \n",
    "\n",
    "        video_path_1 = optical_flow_video\n",
    "        video_path_2 = romp_videos\n",
    "        Skeleton_path = skelton_data\n",
    "\n",
    "        frames_1 = self._load_frames(video_path_1)\n",
    "        frames_2 = self._load_frames_ViVit(video_path_2)\n",
    "        data_3d = self._load_dataframe(Skeleton_path)\n",
    "\n",
    "        video_action = '_'.join(self.video_files_1[idx].split('_')[:2])\n",
    "        label = 0\n",
    "        action_to_label = {\n",
    "            'processed_Arm': 0, 'processed_bs': 1, 'processed_ce': 2, 'processed_dr': 3,\n",
    "            'processed_fg': 4, 'processed_mfs': 5, 'processed_ms': 6,\n",
    "            'processed_sq': 7, 'processed_tw': 8, 'processed_sac': 9, 'processed_tr': 10\n",
    "        }\n",
    "        \n",
    "        label = action_to_label.get(video_action, label)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return frames_1, frames_2, data_3d, label\n",
    "\n",
    "    def _load_frames(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Explicitly convert to grayscale\n",
    "            frame = cv2.resize(frame, (self.frame_width, self.frame_height))  # Resizing the frame\n",
    "            frames.append(frame)\n",
    "            if len(frames) == self.num_frames:\n",
    "                break\n",
    "        cap.release()\n",
    "\n",
    "        # Handle case where video is shorter than num_frames\n",
    "        while len(frames) < self.num_frames:\n",
    "            frames.append(np.zeros((self.frame_height, self.frame_width), dtype=np.float32))  # Gray frame\n",
    "\n",
    "        video_tensor = torch.tensor(np.stack(frames, axis=0)).unsqueeze(1).float() / 255  # Convert to torch tensor and normalize to [0,1]\n",
    "        video_tensor = video_tensor.permute(1, 0, 2, 3)  # Reorder dimensions to [1, 16, 224, 224]\n",
    "\n",
    "        return video_tensor\n",
    "    \n",
    "    def _load_frames_ViVit(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Explicitly convert to grayscale\n",
    "            frame = cv2.resize(frame, (self.frame_width_vivit, self.frame_height_vivit))  # Resizing the frame\n",
    "            frames.append(frame)\n",
    "            if len(frames) == self.num_frames_vivit:\n",
    "                break\n",
    "        cap.release()\n",
    "\n",
    "        # Handle case where video is shorter than num_frames\n",
    "        while len(frames) < self.num_frames_vivit:\n",
    "            frames.append(np.zeros((self.frame_width_vivit, self.frame_height_vivit), dtype=np.float32))  # Gray frame\n",
    "\n",
    "        video_tensor = torch.tensor(np.stack(frames, axis=0)).unsqueeze(1).float() / 255  # Convert to torch tensor and normalize to [0,1]\n",
    "        video_tensor = video_tensor.permute(1, 0, 2, 3)  # Reorder dimensions to [1, 16, 224, 224]\n",
    "\n",
    "        return video_tensor\n",
    "    \n",
    "    \n",
    "    def _load_dataframe(self, lstm_path):\n",
    "        \n",
    "        df = pd.read_csv(lstm_path)\n",
    "        df = df.drop(['Action_Label', 'ASD_Label'], axis = 1)\n",
    "        df_min = df.min().min()\n",
    "        df_max = df.max().max()\n",
    "        \n",
    "        normalized_data = (df - df_min)/(df_max - df_min)\n",
    "        \n",
    "        data_array = normalized_data.values\n",
    "\n",
    "        data_tensor = torch.tensor(data_array, dtype=torch.float)  \n",
    "        return data_tensor\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fbd501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_folder_optical_flow =  'Complete_Files_Optical_Together'\n",
    "video_folder_romp = 'Complete_Romp_Video_All_20_Frames_For_ViVit'\n",
    "lstm_table_data = 'Complete_3D_Skeleton_Data_Star'\n",
    "\n",
    "\n",
    "list_files_optical_flow = os.listdir(video_folder_optical_flow)\n",
    "list_files_lstm_data = os.listdir(lstm_table_data)\n",
    "list_files_romp = os.listdir(video_folder_romp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94bea99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(0, len(list_files_optical_flow)):\n",
    "    list_files_optical_flow[n] = list_files_optical_flow[n].split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13460b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(0, len(list_files_lstm_data)):\n",
    "    list_files_lstm_data[n] = list_files_lstm_data[n].split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dbdfd768",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(0, len(list_files_romp)):\n",
    "    list_files_romp[n] = list_files_romp[n].split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2144c018",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_optical_flow = set(list_files_optical_flow)\n",
    "set_lstm_data = set(list_files_lstm_data)\n",
    "set_romp = set(list_files_romp)\n",
    "\n",
    "common_files = set_optical_flow & set_lstm_data & set_romp\n",
    "\n",
    "common_files_list = list(common_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6f125e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3214"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common_files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fd9591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 40\n",
    "frame_height = 200\n",
    "frame_width = 200\n",
    "frame_height_vivit = 100\n",
    "frame_width_vivit = 100\n",
    "num_frames_vivit = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c6f402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomVideoDataset(video_folders_list=common_files_list, num_frames=num_frames, frame_height=frame_height, frame_width=frame_width, frame_width_vivit = frame_width_vivit,frame_height_vivit = frame_height_vivit, num_frames_vivit = num_frames_vivit )\n",
    "\n",
    "validation_split = 0.2\n",
    "shuffle_dataset = True\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "if shuffle_dataset:\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9df2c093",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm3d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "    \n",
    "# ResNet3D Model\n",
    "class ResNet3D(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=512):\n",
    "        super(ResNet3D, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv3d(1, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm3d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self.make_layer(block, 64, layers[0], stride=1)\n",
    "        self.layer2 = self.make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self.make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self.make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def make_layer(self, block, out_channels, blocks, stride):\n",
    "        strides = [stride] + [1] * (blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "def resnet3d():\n",
    "    return ResNet3D(ResidualBlock, [2, 2, 2, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f02d884",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "cnn_optical = resnet3d().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f22c8449",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropPath(nn.Module):\n",
    "\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return 'p={}'.format(self.drop_prob)\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., attn_head_dim=None):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        if attn_head_dim is not None:\n",
    "            head_dim = attn_head_dim\n",
    "        all_head_dim = head_dim * self.num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n",
    "        if qkv_bias:\n",
    "            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
    "            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
    "        else:\n",
    "            self.q_bias = None\n",
    "            self.v_bias = None\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(all_head_dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv_bias = None\n",
    "        if self.q_bias is not None:\n",
    "            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n",
    "        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n",
    "        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm, attn_head_dim=None):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop, proj_drop=drop, attn_head_dim=attn_head_dim)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        if init_values is not None and init_values > 0:\n",
    "            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "        else:\n",
    "            self.gamma_1, self.gamma_2 = None, None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.gamma_1 is None:\n",
    "            x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        else:\n",
    "            x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x)))\n",
    "            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=100, patch_size=10, in_chans=1, embed_dim=200, num_frames=20, tubelet_size=2):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.tubelet_size = int(tubelet_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0]) * (num_frames // self.tubelet_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.proj = nn.Conv3d(in_channels=in_chans, out_channels=embed_dim,\n",
    "                            kernel_size=(self.tubelet_size, patch_size[0], patch_size[1]),\n",
    "                            stride=(self.tubelet_size, patch_size[0], patch_size[1]))\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        B, C, T, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "def get_sinusoid_encoding_table(n_position, d_hid):\n",
    "    ''' Sinusoid position encoding table '''\n",
    "    def get_position_angle_vec(position):\n",
    "        return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n",
    "\n",
    "    sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
    "\n",
    "    return torch.tensor(sinusoid_table, dtype=torch.float, requires_grad=False).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf1b520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PretrainVisionTransformerEncoder(nn.Module):\n",
    "    def __init__(self, img_size=100, patch_size=10, in_chans=1, num_classes=0, embed_dim=200, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., norm_layer=nn.LayerNorm, init_values=None, tubelet_size=2, use_checkpoint=False,\n",
    "                 use_learnable_pos_emb=False, num_frames=20):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
    "            num_frames=num_frames, tubelet_size=tubelet_size)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        self.pos_embed = get_sinusoid_encoding_table(num_patches, embed_dim)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n",
    "                init_values=init_values)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.pos_embed.type_as(x).to(x.device).clone().detach()\n",
    "        B, _, C = x.shape\n",
    "\n",
    "        if self.use_checkpoint:\n",
    "            for blk in self.blocks:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "                x_vis = x\n",
    "        else:\n",
    "            for blk in self.blocks:\n",
    "                x = blk(x)\n",
    "                x_vis = x\n",
    "\n",
    "        x_vis = self.norm(x_vis)\n",
    "        return x_vis\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        #print(x.shape)\n",
    "        #x = self.head(x)\n",
    "        x = x.mean(dim=1)\n",
    "        #x = self.mlp_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc2008d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 100\n",
    "patch_size = 10\n",
    "in_chans = 1\n",
    "num_classes = 11\n",
    "embed_dim = 512\n",
    "depth = 12\n",
    "num_heads = 12\n",
    "mlp_ratio = 4.0\n",
    "qkv_bias = False\n",
    "qk_scale = None\n",
    "drop_rate = 0.0\n",
    "attn_drop_rate = 0.0\n",
    "drop_path_rate = 0.0\n",
    "norm_layer = nn.LayerNorm\n",
    "init_values = None\n",
    "tubelet_size = 2\n",
    "use_checkpoint = False\n",
    "use_learnable_pos_emb = False\n",
    "num_frames = 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "635cb480",
   "metadata": {},
   "outputs": [],
   "source": [
    "ViVit_Romp = PretrainVisionTransformerEncoder(\n",
    "    img_size=img_size, patch_size=patch_size, in_chans=in_chans, num_classes=num_classes,\n",
    "    embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n",
    "    qk_scale=qk_scale, drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, drop_path_rate=drop_path_rate,\n",
    "    norm_layer=norm_layer, init_values=init_values, tubelet_size=tubelet_size, use_checkpoint=use_checkpoint,\n",
    "    use_learnable_pos_emb=use_learnable_pos_emb, num_frames=num_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b4e8e426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes= 1024):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1024)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "input_size = 75\n",
    "hidden_size = 64\n",
    "num_layers = 4\n",
    "num_classes = 11\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a97d756",
   "metadata": {},
   "outputs": [],
   "source": [
    "Skeleton_Lstm = LSTMModel(input_size, hidden_size, num_layers, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a925d0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, img_size=100, patch_size=10, in_chans=1, num_classes=11, embed_dim=512, depth=12, num_heads=8, mlp_ratio=4.0, qkv_bias=False, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_values=None, tubelet_size=2, use_checkpoint=False, num_frames=20):\n",
    "        super(MultiModalModel, self).__init__()\n",
    "        num_heads = 8\n",
    "        \n",
    "        self.optical_flow_model = cnn_optical  # Pretrained ViViT model\n",
    "        self.romp_model = ViVit_Romp  # Pretrained CNN model\n",
    "        self.lstm_model = Skeleton_Lstm  # Pretrained LSTM model\n",
    "        \n",
    "        optical_flow_dim = 512  # Assuming the output dimension of ViViT model\n",
    "        romp_dim = 512  # Assuming the output dimension of CNN model\n",
    "        lstm_dim = 1024  # Assuming the output dimension of LSTM model\n",
    "        \n",
    "        combined_dim = optical_flow_dim + romp_dim  # Combine the dimensions of optical_flow and romp\n",
    "        print(num_heads)\n",
    "        print(combined_dim)\n",
    "        print(combined_dim/num_heads)\n",
    "        assert combined_dim % num_heads == 0, \"Combined dimension must be divisible by the number of heads.\"\n",
    "        \n",
    "        self.attention = Attention(dim=combined_dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop_rate, proj_drop=drop_rate)\n",
    "        \n",
    "        self.fc = nn.Linear(combined_dim, num_classes)\n",
    "\n",
    "    def forward(self, optical_flow, romp, lstm_skeleton):\n",
    "        optical_flow_features = self.optical_flow_model(optical_flow).unsqueeze(1)  # [B, 1, 512]\n",
    "        romp_features = self.romp_model(romp).unsqueeze(1)  # [B, 1, 512]\n",
    "        lstm_features = self.lstm_model(lstm_skeleton).unsqueeze(1)  # [B, 1, 512]\n",
    "        \n",
    "        combined_features = torch.cat((optical_flow_features, romp_features), dim=-1)  # [B, 1, 1024]\n",
    "        \n",
    "        # Using combined_features as q and v, and lstm_features as k\n",
    "        attended_features = self.attention(query=combined_features, key=lstm_features, value=combined_features)\n",
    "        \n",
    "        logits = self.fc(attended_features)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super(Attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        B, N, C = query.shape\n",
    "        head_dim = C // self.num_heads\n",
    "        assert head_dim * self.num_heads == C, \"embedding dimension must be divisible by num_heads\"\n",
    "        \n",
    "        q = query.reshape(B, N, self.num_heads, head_dim).permute(0, 2, 1, 3)\n",
    "        k = key.reshape(B, N, self.num_heads, head_dim).permute(0, 2, 1, 3)\n",
    "        v = value.reshape(B, N, self.num_heads, head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        \n",
    "        return x.mean(dim=1)  # Reduce along the sequence length dimension\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0261d0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "1024\n",
      "128.0\n"
     ]
    }
   ],
   "source": [
    "model = MultiModalModel(\n",
    "    img_size=img_size, patch_size=patch_size, in_chans=in_chans, num_classes=num_classes,\n",
    "    embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n",
    "    qk_scale=qk_scale, drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, drop_path_rate=drop_path_rate,\n",
    "    norm_layer=norm_layer, init_values=init_values, tubelet_size=tubelet_size, use_checkpoint=use_checkpoint,\n",
    "    num_frames=num_frames\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "52cce72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiModalModel(\n",
       "  (optical_flow_model): ResNet3D(\n",
       "    (conv1): Conv3d(1, 64, kernel_size=(7, 7, 7), stride=(2, 2, 2), padding=(3, 3, 3))\n",
       "    (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool3d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2))\n",
       "          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (conv1): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2))\n",
       "          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (conv1): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2))\n",
       "          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (conv1): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (global_avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (romp_model): PretrainVisionTransformerEncoder(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv3d(1, 512, kernel_size=(2, 10, 10), stride=(2, 10, 10))\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=512, out_features=1512, bias=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=504, out_features=512, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (head): Linear(in_features=512, out_features=11, bias=True)\n",
       "    (mlp_head): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=11, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (lstm_model): LSTMModel(\n",
       "    (lstm): LSTM(75, 64, num_layers=4, batch_first=True)\n",
       "    (fc): Linear(in_features=64, out_features=1024, bias=True)\n",
       "  )\n",
       "  (attention): Attention(\n",
       "    (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=1024, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f6ac172c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100], Train Loss: 0.4215, Validation Loss: 0.1636, Validation Accuracy: 95.64%\n",
      "Epoch [1/100], Train Loss: 0.0751, Validation Loss: 0.1719, Validation Accuracy: 94.86%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(optical, romp, lstm)\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     19\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for optical, romp, lstm, labels in train_loader:\n",
    "        optical, romp, labels = optical.to(device), romp.to(device), labels.to(device)\n",
    "        lstm = lstm.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(optical, romp, lstm)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for optical, romp, lstm, labels in val_loader:\n",
    "            optical, romp, labels = optical.to(device), romp.to(device), labels.to(device)\n",
    "            lstm = lstm.to(device)\n",
    "            outputs = model(optical, romp, lstm)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    \n",
    "    if epoch % 5  == 0:\n",
    "        torch.save(model.state_dict(), 'ViViT_CNN_LSTM/' + f'combined_model_epoch_ViViT(ROMP)_CNN(OPTICAL_FLOW)_{epoch}.pth')\n",
    "\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}], Train Loss: {train_loss:.4f}, \"\n",
    "          f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ab99dd92",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 27\u001b[0m\n\u001b[1;32m     21\u001b[0m         all_predictions\u001b[38;5;241m.\u001b[39mextend(predicted\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Generate the classification report\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m report \u001b[38;5;241m=\u001b[39m classification_report(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, all_predictions, target_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArm_Swing\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     28\u001b[0m                                                                          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBody_pose\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     29\u001b[0m                                                                           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDrumming\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     30\u001b[0m                                                                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrog_Pose\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     31\u001b[0m                                                                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMarcas_Forward\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     32\u001b[0m                                                                          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMarcas_Shaking\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     33\u001b[0m                                                                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSing_Clap\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     34\u001b[0m                                                                          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSquat_Pose\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     35\u001b[0m                                                                          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTree_Pose\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     36\u001b[0m                                                                           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTwist_Pose\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     37\u001b[0m                                                                          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchest_expansion\u001b[39m\u001b[38;5;124m'\u001b[39m ])\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(report)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming the following variables are already defined:\n",
    "# combined_model_with_attention, val_loader, device, action_to_label\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "# No need to compute gradients for evaluation\n",
    "with torch.no_grad():\n",
    "    for optical_flow, romp, lstm, labels in val_loader:\n",
    "        optical_flow, romp, lstm,  labels = optical_flow.to(device),  romp.to(device), lstm.to(device), labels.to(device)\n",
    "        outputs = model(optical_flow, romp,  lstm )\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "04775b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Arm_Swing       1.00      0.95      0.97        37\n",
      "      Body_pose       1.00      1.00      1.00        53\n",
      "       Drumming       0.96      0.98      0.97        48\n",
      "      Frog_Pose       0.98      0.94      0.96        87\n",
      " Marcas_Forward       0.97      1.00      0.98        62\n",
      " Marcas_Shaking       0.91      0.96      0.93        67\n",
      "      Sing_Clap       0.96      0.95      0.96        80\n",
      "     Squat_Pose       1.00      0.92      0.96        37\n",
      "      Tree_Pose       0.96      1.00      0.98        44\n",
      "     Twist_Pose       0.95      0.98      0.96        54\n",
      "chest_expansion       1.00      0.99      0.99        73\n",
      "\n",
      "       accuracy                           0.97       642\n",
      "      macro avg       0.97      0.97      0.97       642\n",
      "   weighted avg       0.97      0.97      0.97       642\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate the classification report\n",
    "report = classification_report(np.array(all_labels), all_predictions, target_names=['Arm_Swing',\n",
    "                                                                         'Body_pose',\n",
    "                                                                          'Drumming',\n",
    "                                                                        'Frog_Pose',\n",
    "                                                                        'Marcas_Forward',\n",
    "                                                                         'Marcas_Shaking',\n",
    "                                                                        'Sing_Clap',\n",
    "                                                                         'Squat_Pose',\n",
    "                                                                         'Tree_Pose',\n",
    "                                                                          'Twist_Pose',\n",
    "                                                                         'chest_expansion' ])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1f8e4007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.9688\n",
      "Class Accuracies:\n",
      "Arm_Swing: 1.0000\n",
      "Body_pose: 1.0000\n",
      "Drumming: 0.9592\n",
      "Frog_Pose: 0.9762\n",
      "Marcas_Forward: 0.9688\n",
      "Marcas_Shaking: 0.9143\n",
      "Sing_Clap: 0.9620\n",
      "Squat_Pose: 1.0000\n",
      "Tree_Pose: 0.9565\n",
      "Twist_Pose: 0.9464\n",
      "chest_expansion: 1.0000\n",
      "macro avg: 0.9712\n",
      "weighted avg: 0.9695\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "report_dict = classification_report(np.array(all_labels), all_predictions, target_names=['Arm_Swing',\n",
    "                                                                         'Body_pose',\n",
    "                                                                         'Drumming',\n",
    "                                                                         'Frog_Pose',\n",
    "                                                                         'Marcas_Forward',\n",
    "                                                                         'Marcas_Shaking',\n",
    "                                                                         'Sing_Clap',\n",
    "                                                                         'Squat_Pose',\n",
    "                                                                         'Tree_Pose',\n",
    "                                                                         'Twist_Pose',\n",
    "                                                                         'chest_expansion'], output_dict=True)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the overall accuracy\n",
    "accuracy = accuracy_score(np.array(all_labels), all_predictions)\n",
    "print(f'Overall Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Extract accuracy values from the report dictionary\n",
    "class_accuracies = {class_name: metrics['precision'] for class_name, metrics in report_dict.items() if class_name != 'accuracy'}\n",
    "print('Class Accuracies:')\n",
    "for class_name, acc in class_accuracies.items():\n",
    "    print(f'{class_name}: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a6fff4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Metrics:\n",
      "Arm_Swing: Precision: 1.0000, Recall: 0.9459, F1-Score: 0.9722\n",
      "Body_pose: Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000\n",
      "Drumming: Precision: 0.9592, Recall: 0.9792, F1-Score: 0.9691\n",
      "Frog_Pose: Precision: 0.9762, Recall: 0.9425, F1-Score: 0.9591\n",
      "Marcas_Forward: Precision: 0.9688, Recall: 1.0000, F1-Score: 0.9841\n",
      "Marcas_Shaking: Precision: 0.9143, Recall: 0.9552, F1-Score: 0.9343\n",
      "Sing_Clap: Precision: 0.9620, Recall: 0.9500, F1-Score: 0.9560\n",
      "Squat_Pose: Precision: 1.0000, Recall: 0.9189, F1-Score: 0.9577\n",
      "Tree_Pose: Precision: 0.9565, Recall: 1.0000, F1-Score: 0.9778\n",
      "Twist_Pose: Precision: 0.9464, Recall: 0.9815, F1-Score: 0.9636\n",
      "chest_expansion: Precision: 1.0000, Recall: 0.9863, F1-Score: 0.9931\n",
      "macro avg: Precision: 0.9712, Recall: 0.9691, F1-Score: 0.9697\n",
      "weighted avg: Precision: 0.9695, Recall: 0.9688, F1-Score: 0.9689\n"
     ]
    }
   ],
   "source": [
    "# Extract precision, recall, f1-score for each class\n",
    "class_metrics = {class_name: {'precision': metrics['precision'], \n",
    "                              'recall': metrics['recall'], \n",
    "                              'f1-score': metrics['f1-score']} \n",
    "                 for class_name, metrics in report_dict.items() if class_name != 'accuracy'}\n",
    "\n",
    "print('Class Metrics:')\n",
    "for class_name, metrics in class_metrics.items():\n",
    "    print(f'{class_name}: Precision: {metrics[\"precision\"]:.4f}, Recall: {metrics[\"recall\"]:.4f}, F1-Score: {metrics[\"f1-score\"]:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8dd57a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
