{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9cd69b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.0.7)\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from timm) (2.2.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from timm) (0.17.0)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from timm) (0.24.2)\n",
      "Requirement already satisfied: safetensors in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from timm) (0.4.3)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub->timm) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub->timm) (2024.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub->timm) (21.3)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub->timm) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub->timm) (4.12.1)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->timm) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->timm) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->timm) (3.1.4)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torchvision->timm) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torchvision->timm) (10.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub->timm) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch->timm) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: tensorboardX in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.6.2.2)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboardX) (1.26.4)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboardX) (21.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboardX) (4.25.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging->tensorboardX) (3.1.2)\n",
      "Requirement already satisfied: einops in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install timm\n",
    "!pip install tensorboardX\n",
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73d0d513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers import drop_path, to_2tuple, trunc_normal_\n",
    "from timm.models.registry import register_model\n",
    "import torch.utils.checkpoint as checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2110217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVideoDataset(Dataset):\n",
    "    def __init__(self, video_folders_list, num_frames, frame_height, frame_width):\n",
    "        self.video_files_1 =  video_folders_list\n",
    "    \n",
    "        self.num_frames = num_frames\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_files_1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        optical_flow_video = 'Complete_Optical_Flow_20_Frames_ViViT_Star' +  '/' + self.video_files_1[idx]\n",
    "        romp_videos = 'Complete_Romp_Video_All_20_Frames_For_ViVit' + '/' + self.video_files_1[idx]\n",
    "\n",
    "        video_path_1 = optical_flow_video\n",
    "        video_path_2 = romp_videos\n",
    "\n",
    "        frames_1 = self._load_frames(video_path_1)\n",
    "        frames_2 = self._load_frames(video_path_2)\n",
    "\n",
    "        video_action = '_'.join(self.video_files_1[idx].split('_')[:2])\n",
    "        label = 0\n",
    "        action_to_label = {\n",
    "            'processed_Arm': 0, 'processed_bs': 1, 'processed_ce': 2, 'processed_dr': 3,\n",
    "            'processed_fg': 4, 'processed_mfs': 5, 'processed_ms': 6,\n",
    "            'processed_sq': 7, 'processed_tw': 8, 'processed_sac': 9, 'processed_tr': 10\n",
    "        }\n",
    "        \n",
    "        label = action_to_label.get(video_action, label)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return frames_1, frames_2, label\n",
    "\n",
    "    def _load_frames(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Explicitly convert to grayscale\n",
    "            frame = cv2.resize(frame, (self.frame_width, self.frame_height))  # Resizing the frame\n",
    "            frames.append(frame)\n",
    "            if len(frames) == self.num_frames:\n",
    "                break\n",
    "        cap.release()\n",
    "\n",
    "        while len(frames) < self.num_frames:\n",
    "            frames.append(np.zeros((self.frame_height, self.frame_width), dtype=np.float32))  # Gray frame\n",
    "\n",
    "        video_tensor = torch.tensor(np.stack(frames, axis=0)).unsqueeze(1).float() / 255  # Convert to torch tensor and normalize to [0,1]\n",
    "        video_tensor = video_tensor.permute(1, 0, 2, 3)  # Reorder dimensions to [1, 16, 224, 224]\n",
    "\n",
    "        return video_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cebb2c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_folder_optical_flow =  'Complete_Optical_Flow_20_Frames_ViViT_Star_New'\n",
    "video_romp_video = 'Complete_Romp_Video_All_20_Frames_For_ViVit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54c883e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files_optical_flow = os.listdir(video_folder_optical_flow)\n",
    "list_files_romp_video = os.listdir(video_romp_video)\n",
    "final_video_files =   set(list_files_romp_video) - set(list_files_optical_flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "506c0127",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_final_list = []\n",
    "for n in range(0, len(list_files_romp_video)):\n",
    "    if list_files_romp_video[n] in list_files_optical_flow:\n",
    "        complete_final_list.append(list_files_romp_video[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9f44f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3215"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(complete_final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cb6460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90482656",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 20\n",
    "frame_height = 100\n",
    "frame_width = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53188306",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomVideoDataset(video_folders_list=complete_final_list, num_frames=num_frames, frame_height=frame_height, frame_width=frame_width)\n",
    "\n",
    "validation_split = 0.2\n",
    "shuffle_dataset = True\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "if shuffle_dataset:\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4eec2928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropPath(nn.Module):\n",
    "\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return 'p={}'.format(self.drop_prob)\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., attn_head_dim=None):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        if attn_head_dim is not None:\n",
    "            head_dim = attn_head_dim\n",
    "        all_head_dim = head_dim * self.num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n",
    "        if qkv_bias:\n",
    "            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
    "            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
    "        else:\n",
    "            self.q_bias = None\n",
    "            self.v_bias = None\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(all_head_dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv_bias = None\n",
    "        if self.q_bias is not None:\n",
    "            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n",
    "        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n",
    "        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm, attn_head_dim=None):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop, proj_drop=drop, attn_head_dim=attn_head_dim)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        if init_values is not None and init_values > 0:\n",
    "            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "        else:\n",
    "            self.gamma_1, self.gamma_2 = None, None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.gamma_1 is None:\n",
    "            x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        else:\n",
    "            x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x)))\n",
    "            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=100, patch_size=10, in_chans=1, embed_dim=200, num_frames=20, tubelet_size=2):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.tubelet_size = int(tubelet_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0]) * (num_frames // self.tubelet_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.proj = nn.Conv3d(in_channels=in_chans, out_channels=embed_dim,\n",
    "                            kernel_size=(self.tubelet_size, patch_size[0], patch_size[1]),\n",
    "                            stride=(self.tubelet_size, patch_size[0], patch_size[1]))\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        B, C, T, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "def get_sinusoid_encoding_table(n_position, d_hid):\n",
    "    ''' Sinusoid position encoding table '''\n",
    "    def get_position_angle_vec(position):\n",
    "        return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n",
    "\n",
    "    sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
    "\n",
    "    return torch.tensor(sinusoid_table, dtype=torch.float, requires_grad=False).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "529b521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PretrainVisionTransformerEncoder(nn.Module):\n",
    "    def __init__(self, img_size=100, patch_size=10, in_chans=1, num_classes=0, embed_dim=200, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., norm_layer=nn.LayerNorm, init_values=None, tubelet_size=2, use_checkpoint=False,\n",
    "                 use_learnable_pos_emb=False, num_frames=20):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
    "            num_frames=num_frames, tubelet_size=tubelet_size)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        self.pos_embed = get_sinusoid_encoding_table(num_patches, embed_dim)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n",
    "                init_values=init_values)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.pos_embed.type_as(x).to(x.device).clone().detach()\n",
    "        B, _, C = x.shape\n",
    "\n",
    "        if self.use_checkpoint:\n",
    "            for blk in self.blocks:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "                x_vis = x\n",
    "        else:\n",
    "            for blk in self.blocks:\n",
    "                x = blk(x)\n",
    "                x_vis = x\n",
    "\n",
    "        x_vis = self.norm(x_vis)\n",
    "        return x_vis\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        #print(x.shape)\n",
    "        #x = self.head(x)\n",
    "        x = x.mean(dim=1)\n",
    "        #x = self.mlp_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da14c574",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 100\n",
    "patch_size = 10\n",
    "in_chans = 1\n",
    "num_classes = 11\n",
    "embed_dim = 400\n",
    "depth = 12\n",
    "num_heads = 12\n",
    "mlp_ratio = 4.0\n",
    "qkv_bias = False\n",
    "qk_scale = None\n",
    "drop_rate = 0.0\n",
    "attn_drop_rate = 0.0\n",
    "drop_path_rate = 0.0\n",
    "norm_layer = nn.LayerNorm\n",
    "init_values = None\n",
    "tubelet_size = 2\n",
    "use_checkpoint = False\n",
    "use_learnable_pos_emb = False\n",
    "num_frames = 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ce29d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Optical_Flow  = PretrainVisionTransformerEncoder(\n",
    "    img_size=img_size, patch_size=patch_size, in_chans=in_chans, num_classes=num_classes,\n",
    "    embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n",
    "    qk_scale=qk_scale, drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, drop_path_rate=drop_path_rate,\n",
    "    norm_layer=norm_layer, init_values=init_values, tubelet_size=tubelet_size, use_checkpoint=use_checkpoint,\n",
    "    use_learnable_pos_emb=use_learnable_pos_emb, num_frames=num_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb6b0587",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Romp_Video = PretrainVisionTransformerEncoder(\n",
    "    img_size=img_size, patch_size=patch_size, in_chans=in_chans, num_classes=num_classes,\n",
    "    embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n",
    "    qk_scale=qk_scale, drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, drop_path_rate=drop_path_rate,\n",
    "    norm_layer=norm_layer, init_values=init_values, tubelet_size=tubelet_size, use_checkpoint=use_checkpoint,\n",
    "    use_learnable_pos_emb=use_learnable_pos_emb, num_frames=num_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "26fd0063",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, img_size=100, patch_size=10, in_chans=1, num_classes=11, embed_dim=200, depth=12, num_heads=12, mlp_ratio=4.0, qkv_bias=False, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_values=None, tubelet_size=2, use_checkpoint=False, num_frames=20):\n",
    "        super(MultiModalModel, self).__init__()\n",
    "        self.optical_flow_model = model_Optical_Flow\n",
    "        self.romp_model = model_Romp_Video\n",
    "        self.attention = Attention(dim=embed_dim*2, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop_rate, proj_drop=drop_rate)\n",
    "        self.fc = nn.Linear(embed_dim*2, num_classes)\n",
    "\n",
    "    def forward(self, optical_flow, romp):\n",
    "        optical_flow_features = self.optical_flow_model(optical_flow)\n",
    "        romp_features = self.romp_model(romp)\n",
    "        combined_features = torch.cat((optical_flow_features, romp_features), dim=-1)\n",
    "        attended_features = self.attention(combined_features.unsqueeze(1)).squeeze(1)\n",
    "        logits = self.fc(attended_features)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "310d4b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiModalModel(\n",
    "    img_size=img_size, patch_size=patch_size, in_chans=in_chans, num_classes=num_classes,\n",
    "    embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n",
    "    qk_scale=qk_scale, drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, drop_path_rate=drop_path_rate,\n",
    "    norm_layer=norm_layer, init_values=init_values, tubelet_size=tubelet_size, use_checkpoint=use_checkpoint,\n",
    "    num_frames=num_frames\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "981d9a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiModalModel(\n",
       "  (optical_flow_model): PretrainVisionTransformerEncoder(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv3d(1, 400, kernel_size=(2, 10, 10), stride=(2, 10, 10))\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (norm1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=400, out_features=1188, bias=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=396, out_features=400, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=400, out_features=1600, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1600, out_features=400, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "    (head): Linear(in_features=400, out_features=11, bias=True)\n",
       "    (mlp_head): Sequential(\n",
       "      (0): Linear(in_features=400, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=11, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (romp_model): PretrainVisionTransformerEncoder(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv3d(1, 400, kernel_size=(2, 10, 10), stride=(2, 10, 10))\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (norm1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=400, out_features=1188, bias=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=396, out_features=400, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=400, out_features=1600, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1600, out_features=400, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "    (head): Linear(in_features=400, out_features=11, bias=True)\n",
       "    (mlp_head): Sequential(\n",
       "      (0): Linear(in_features=400, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=11, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (attention): Attention(\n",
       "    (qkv): Linear(in_features=800, out_features=2376, bias=False)\n",
       "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "    (proj): Linear(in_features=792, out_features=800, bias=True)\n",
       "    (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=800, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "950c9d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_classes = 11\n",
    "num_epochs = 70\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7d57e023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a849f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/5], Train Loss: 0.3523, Validation Loss: 0.1696, Validation Accuracy: 94.87%\n",
      "Epoch [1/5], Train Loss: 0.0679, Validation Loss: 0.1332, Validation Accuracy: 95.49%\n",
      "Epoch [2/5], Train Loss: 0.1306, Validation Loss: 0.2740, Validation Accuracy: 92.53%\n",
      "Epoch [3/5], Train Loss: 0.0696, Validation Loss: 0.2911, Validation Accuracy: 90.67%\n",
      "Epoch [4/5], Train Loss: 0.0549, Validation Loss: 0.2089, Validation Accuracy: 93.31%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for videos1, videos2, labels in train_loader:\n",
    "        videos1, videos2, labels = videos1.to(device), videos2.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(videos1, videos2)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for videos1, videos2, labels in val_loader:\n",
    "            videos1, videos2, labels = videos1.to(device), videos2.to(device), labels.to(device)\n",
    "            outputs = model(videos1, videos2)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    \n",
    "    #if epoch %  == 0:\n",
    "    torch.save(model.state_dict(), 'ViViT_Combined_Weights_Optical_Flow_Romp_Videos/' + f'combined_model_epoch_{epoch}.pth')\n",
    "\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}], Train Loss: {train_loss:.4f}, \"\n",
    "          f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dfa9c1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9c8b416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for videos1, videos2, labels in val_loader:\n",
    "        videos1, videos2, labels = videos1.to(device), videos2.to(device), labels.to(device)\n",
    "        outputs = model(videos1, videos2)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "428d6b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "processed_Arm       0.91      0.97      0.94        30\n",
      " processed_bs       0.98      0.92      0.95        62\n",
      " processed_ce       0.92      0.86      0.89        51\n",
      " processed_dr       0.99      0.90      0.94        83\n",
      " processed_fg       0.88      0.97      0.92        59\n",
      "processed_mfs       0.97      0.97      0.97        60\n",
      " processed_ms       0.83      1.00      0.91        74\n",
      " processed_sq       0.95      1.00      0.97        38\n",
      " processed_tw       0.92      0.92      0.92        64\n",
      "processed_sac       0.95      0.80      0.87        51\n",
      " processed_tr       1.00      0.96      0.98        71\n",
      "\n",
      "     accuracy                           0.93       643\n",
      "    macro avg       0.94      0.93      0.93       643\n",
      " weighted avg       0.94      0.93      0.93       643\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "report = classification_report(all_labels, all_predictions, target_names=['processed_Arm', 'processed_bs', 'processed_ce', 'processed_dr',\n",
    "    'processed_fg', 'processed_mfs', 'processed_ms',\n",
    "    'processed_sq', 'processed_tw', 'processed_sac', 'processed_tr'])\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8051ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
