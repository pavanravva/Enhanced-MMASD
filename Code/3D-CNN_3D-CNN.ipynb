{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8beaeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, TensorDataset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ad9c420",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVideoDataset(Dataset):\n",
    "    def __init__(self, video_folders_list, num_frames, frame_height, frame_width):\n",
    "        self.video_files_1 =  video_folders_list    \n",
    "        self.num_frames = num_frames\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_files_1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        optical_flow_video = 'Complete_Files_Optical_Together' +  '/' + self.video_files_1[idx]\n",
    "        romp_videos = 'Complete_Files_Romp_Together' + '/' + self.video_files_1[idx]\n",
    "\n",
    "        video_path_1 = optical_flow_video\n",
    "        video_path_2 = romp_videos\n",
    "\n",
    "        frames_1 = self._load_frames(video_path_1)\n",
    "        frames_2 = self._load_frames(video_path_2)\n",
    "\n",
    "        video_action = '_'.join(self.video_files_1[idx].split('_')[:2])\n",
    "        label = 0\n",
    "        action_to_label = {\n",
    "            'processed_Arm': 0, 'processed_bs': 1, 'processed_ce': 2, 'processed_dr': 3,\n",
    "            'processed_fg': 4, 'processed_mfs': 5, 'processed_ms': 6,\n",
    "            'processed_sq': 7, 'processed_tw': 8, 'processed_sac': 9, 'processed_tr': 10\n",
    "        }\n",
    "        \n",
    "        label = action_to_label.get(video_action, label)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return frames_1, frames_2, label\n",
    "\n",
    "    def _load_frames(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Explicitly convert to grayscale\n",
    "            frame = cv2.resize(frame, (self.frame_width, self.frame_height))  # Resizing the frame\n",
    "            frames.append(frame)\n",
    "            if len(frames) == self.num_frames:\n",
    "                break\n",
    "        cap.release()\n",
    "\n",
    "        # Handle case where video is shorter than num_frames\n",
    "        while len(frames) < self.num_frames:\n",
    "            frames.append(np.zeros((self.frame_height, self.frame_width), dtype=np.float32))  # Gray frame\n",
    "\n",
    "        video_tensor = torch.tensor(np.stack(frames, axis=0)).unsqueeze(1).float() / 255  # Convert to torch tensor and normalize to [0,1]\n",
    "        video_tensor = video_tensor.permute(1, 0, 2, 3)  # Reorder dimensions to [1, 16, 224, 224]\n",
    "\n",
    "        return video_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c7edf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_folder_optical_flow =  'Complete_Files_Optical_Together'\n",
    "video_romp_video = 'Complete_Files_Romp_Together'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8de71327",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files_optical_flow = os.listdir(video_folder_optical_flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04c4408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files_romp_video = os.listdir(video_romp_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "401cf030",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_video_files =   set(list_files_romp_video) - set(list_files_optical_flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f307838",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_final_list = []\n",
    "for n in range(0, len(list_files_romp_video)):\n",
    "    if list_files_romp_video[n] in list_files_optical_flow:\n",
    "        complete_final_list.append(list_files_romp_video[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7d32838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3216"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(complete_final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c234d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 40\n",
    "frame_height = 200\n",
    "frame_width = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0ce670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomVideoDataset(video_folders_list=complete_final_list, num_frames=num_frames, frame_height=frame_height, frame_width=frame_width)\n",
    "\n",
    "validation_split = 0.2\n",
    "shuffle_dataset = True\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "if shuffle_dataset:\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8f33d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm3d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# ResNet3D Model\n",
    "class ResNet3D(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=512):\n",
    "        super(ResNet3D, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv3d(1, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm3d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self.make_layer(block, 64, layers[0], stride=1)\n",
    "        self.layer2 = self.make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self.make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self.make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def make_layer(self, block, out_channels, blocks, stride):\n",
    "        strides = [stride] + [1] * (blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "def resnet3d():\n",
    "    return ResNet3D(ResidualBlock, [2, 2, 2, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "896656e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de69f9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two ResNet3D models\n",
    "model1_Optical_Flow= resnet3d().to(device)\n",
    "model2_Romp = resnet3d().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc49abcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, num_heads):\n",
    "        super(Attention, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=feature_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.layer_norm = nn.LayerNorm(feature_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.multihead_attn(x, x, x)\n",
    "        attn_output = self.layer_norm(attn_output + x)\n",
    "        return attn_output.mean(dim=1)  # Aggregate across the sequence dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cf12cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModelWithAttention(nn.Module):\n",
    "    def __init__(self, model1, model2, feature_dim=512, num_heads=8, num_classes=11):\n",
    "        super(CombinedModelWithAttention, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.attention = Attention(feature_dim * 2, num_heads)  # feature_dim * 2 because of concatenation\n",
    "        self.fc_combined = nn.Linear(feature_dim * 2, num_classes)  # Assuming the output of each model is a 512-dimensional feature vector\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        features1 = self.model1(x1)  # [batch_size, 512]\n",
    "        features2 = self.model2(x2)  # [batch_size, 512]\n",
    "        combined_features = torch.cat((features1, features2), dim=1) # [batch_size, 1024]\n",
    "        combined_features = combined_features.unsqueeze(1)  # [batch_size, 1, 1024]\n",
    "        attended_features = self.attention(combined_features)  # [batch_size, 1024]\n",
    "        out = self.fc_combined(attended_features)  # [batch_size, num_classes]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4af7b1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model_with_attention = CombinedModelWithAttention(model1_Optical_Flow, model2_Romp).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8cece769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/5], Train Loss: 0.1445, Validation Loss: 0.0404, Validation Accuracy: 98.60%\n",
      "Epoch [1/5], Train Loss: 0.0148, Validation Loss: 0.0338, Validation Accuracy: 99.07%\n",
      "Epoch [2/5], Train Loss: 0.0132, Validation Loss: 0.0239, Validation Accuracy: 99.38%\n",
      "Epoch [3/5], Train Loss: 0.0536, Validation Loss: 0.0355, Validation Accuracy: 98.60%\n",
      "Epoch [4/5], Train Loss: 0.0572, Validation Loss: 0.0721, Validation Accuracy: 97.67%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(combined_model_with_attention.parameters(), lr=0.0001)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    combined_model_with_attention.train()\n",
    "    running_loss = 0.0\n",
    "    for videos1, videos2, labels in train_loader:\n",
    "        videos1, videos2, labels = videos1.to(device), videos2.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = combined_model_with_attention(videos1, videos2)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    combined_model_with_attention.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for videos1, videos2, labels in val_loader:\n",
    "            videos1, videos2, labels = videos1.to(device), videos2.to(device), labels.to(device)\n",
    "            outputs = combined_model_with_attention(videos1, videos2)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f\"Epoch [{epoch}/{num_epochs}], Train Loss: {train_loss:.4f}, \"\n",
    "          f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15716728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f29dbf3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'action_to_label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m         all_predictions\u001b[38;5;241m.\u001b[39mextend(predicted\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Generate the classification report\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m report \u001b[38;5;241m=\u001b[39m classification_report(all_labels, all_predictions, target_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[43maction_to_label\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(report)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'action_to_label' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming the following variables are already defined:\n",
    "# combined_model_with_attention, val_loader, device, action_to_label\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "combined_model_with_attention.eval()\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "# No need to compute gradients for evaluation\n",
    "with torch.no_grad():\n",
    "    for videos1, videos2, labels in val_loader:\n",
    "        videos1, videos2, labels = videos1.to(device), videos2.to(device), labels.to(device)\n",
    "        outputs = combined_model_with_attention(videos1, videos2)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "691f0521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "processed_Arm       1.00      1.00      1.00        41\n",
      " processed_bs       1.00      0.94      0.97        49\n",
      " processed_ce       0.95      1.00      0.98        59\n",
      " processed_dr       0.95      0.99      0.97        89\n",
      " processed_fg       1.00      0.98      0.99        46\n",
      "processed_mfs       0.90      1.00      0.95        45\n",
      " processed_ms       0.99      0.92      0.95        76\n",
      " processed_sq       1.00      1.00      1.00        36\n",
      " processed_tw       1.00      1.00      1.00        70\n",
      "processed_sac       0.98      0.96      0.97        56\n",
      " processed_tr       1.00      0.97      0.99        76\n",
      "\n",
      "     accuracy                           0.98       643\n",
      "    macro avg       0.98      0.98      0.98       643\n",
      " weighted avg       0.98      0.98      0.98       643\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate the classification report\n",
    "report = classification_report(all_labels, all_predictions, target_names=['processed_Arm', 'processed_bs', 'processed_ce', 'processed_dr',\n",
    "    'processed_fg', 'processed_mfs', 'processed_ms',\n",
    "    'processed_sq', 'processed_tw', 'processed_sac', 'processed_tr'])\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3467ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
