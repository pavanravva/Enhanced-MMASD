{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8bbeb32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8930b4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install timm\n",
    "!pip install tensorboardX\n",
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d48f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, TensorDataset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from timm.models.layers import drop_path, to_2tuple, trunc_normal_\n",
    "from timm.models.registry import register_model\n",
    "import torch.utils.checkpoint as checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "718163a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVideoDataset(Dataset):\n",
    "    def __init__(self, video_folders_list, num_frames, frame_height, frame_width):\n",
    "        \n",
    "        self.video_files_1 =  video_folders_list\n",
    "    \n",
    "        self.num_frames = num_frames\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_files_1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        optical_flow_video = 'Complete_Files_Romp_Together' +  '/' + self.video_files_1[idx] + '.mp4'\n",
    "        Lstm_Data_Normalized = 'Complete_3D_Skeleton_Data_Star' + '/' + self.video_files_1[idx] + '.csv'\n",
    "\n",
    "        video_path_1 = optical_flow_video\n",
    "        lstm_path = Lstm_Data_Normalized\n",
    "\n",
    "        frames_1 = self._load_frames(video_path_1)\n",
    "        lstm_data = self._load_dataframe(lstm_path)\n",
    "\n",
    "        \n",
    "        video_action = '_'.join(self.video_files_1[idx].split('_')[:2])\n",
    "        label = 0\n",
    "        action_to_label = {\n",
    "            'processed_Arm': 0, 'processed_bs': 1, 'processed_ce': 2, 'processed_dr': 3,\n",
    "            'processed_fg': 4, 'processed_mfs': 5, 'processed_ms': 6,\n",
    "            'processed_sq': 7, 'processed_tw': 8, 'processed_sac': 9, 'processed_tr': 10\n",
    "        }\n",
    "        \n",
    "        label = action_to_label.get(video_action, label)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return frames_1, lstm_data, label\n",
    "\n",
    "    def _load_frames(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Explicitly convert to grayscale\n",
    "            frame = cv2.resize(frame, (self.frame_width, self.frame_height))  # Resizing the frame\n",
    "            frames.append(frame)\n",
    "            if len(frames) == self.num_frames:\n",
    "                break\n",
    "        cap.release()\n",
    "\n",
    "        # Handle case where video is shorter than num_frames\n",
    "        while len(frames) < self.num_frames:\n",
    "            frames.append(np.zeros((self.frame_height, self.frame_width), dtype=np.float32))  # Gray frame\n",
    "\n",
    "        video_tensor = torch.tensor(np.stack(frames, axis=0)).unsqueeze(1).float() / 255  # Convert to torch tensor and normalize to [0,1]\n",
    "        video_tensor = video_tensor.permute(1, 0, 2, 3)  # Reorder dimensions to [1, 16, 224, 224]\n",
    "\n",
    "        return video_tensor\n",
    "    \n",
    "    def _load_dataframe(self, lstm_path):\n",
    "        \n",
    "        df = pd.read_csv(lstm_path)\n",
    "        df = df.drop(['Action_Label', 'ASD_Label'], axis = 1)\n",
    "        df_min = df.min().min()\n",
    "        df_max = df.max().max()\n",
    "        \n",
    "        normalized_data = (df - df_min)/(df_max - df_min)\n",
    "        \n",
    "        data_array = normalized_data.values\n",
    "\n",
    "        data_tensor = torch.tensor(data_array, dtype=torch.float)  \n",
    "        return data_tensor\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54b297b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_folder_optical_flow =  'Complete_Files_Romp_Together'\n",
    "lstm_table_data = 'Complete_3D_Skeleton_Data_Star'\n",
    "\n",
    "list_files_optical_flow = os.listdir(video_folder_optical_flow)\n",
    "list_files_lstm_data = os.listdir(lstm_table_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d67bee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files_optical_flow.remove('.ipynb_checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ce0eed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_final_list = []\n",
    "for n in range(0, len(list_files_optical_flow)):\n",
    "    if list_files_optical_flow[n].split('.')[0]+'.csv' in list_files_lstm_data:\n",
    "        complete_final_list.append(list_files_optical_flow[n].split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73969495",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 20\n",
    "frame_height = 100\n",
    "frame_width = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4173896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomVideoDataset(video_folders_list=complete_final_list, num_frames=num_frames, frame_height=frame_height, frame_width=frame_width)\n",
    "\n",
    "validation_split = 0.2\n",
    "shuffle_dataset = True\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "if shuffle_dataset:\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab515f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e011ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes= 512):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 512)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "input_size = 75\n",
    "hidden_size = 64\n",
    "num_layers = 4\n",
    "num_classes = 11\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f45fc3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f49e69a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropPath(nn.Module):\n",
    "\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return 'p={}'.format(self.drop_prob)\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., attn_head_dim=None):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        if attn_head_dim is not None:\n",
    "            head_dim = attn_head_dim\n",
    "        all_head_dim = head_dim * self.num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n",
    "        if qkv_bias:\n",
    "            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
    "            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
    "        else:\n",
    "            self.q_bias = None\n",
    "            self.v_bias = None\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(all_head_dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv_bias = None\n",
    "        if self.q_bias is not None:\n",
    "            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n",
    "        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n",
    "        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm, attn_head_dim=None):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop, proj_drop=drop, attn_head_dim=attn_head_dim)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        if init_values is not None and init_values > 0:\n",
    "            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "        else:\n",
    "            self.gamma_1, self.gamma_2 = None, None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.gamma_1 is None:\n",
    "            x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        else:\n",
    "            x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x)))\n",
    "            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=100, patch_size=10, in_chans=1, embed_dim=200, num_frames=20, tubelet_size=2):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.tubelet_size = int(tubelet_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0]) * (num_frames // self.tubelet_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.proj = nn.Conv3d(in_channels=in_chans, out_channels=embed_dim,\n",
    "                            kernel_size=(self.tubelet_size, patch_size[0], patch_size[1]),\n",
    "                            stride=(self.tubelet_size, patch_size[0], patch_size[1]))\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        B, C, T, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "def get_sinusoid_encoding_table(n_position, d_hid):\n",
    "    ''' Sinusoid position encoding table '''\n",
    "    def get_position_angle_vec(position):\n",
    "        return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n",
    "\n",
    "    sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
    "\n",
    "    return torch.tensor(sinusoid_table, dtype=torch.float, requires_grad=False).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a5eb0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PretrainVisionTransformerEncoder(nn.Module):\n",
    "    def __init__(self, img_size=100, patch_size=10, in_chans=1, num_classes=0, embed_dim=200, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., norm_layer=nn.LayerNorm, init_values=None, tubelet_size=2, use_checkpoint=False,\n",
    "                 use_learnable_pos_emb=False, num_frames=20):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
    "            num_frames=num_frames, tubelet_size=tubelet_size)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        self.pos_embed = get_sinusoid_encoding_table(num_patches, embed_dim)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n",
    "                init_values=init_values)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.pos_embed.type_as(x).to(x.device).clone().detach()\n",
    "        B, _, C = x.shape\n",
    "\n",
    "        if self.use_checkpoint:\n",
    "            for blk in self.blocks:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "                x_vis = x\n",
    "        else:\n",
    "            for blk in self.blocks:\n",
    "                x = blk(x)\n",
    "                x_vis = x\n",
    "\n",
    "        x_vis = self.norm(x_vis)\n",
    "        return x_vis\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        #print(x.shape)\n",
    "        #x = self.head(x)\n",
    "        x = x.mean(dim=1)\n",
    "        #x = self.mlp_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cc05846",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 100\n",
    "patch_size = 10\n",
    "in_chans = 1\n",
    "num_classes = 11\n",
    "embed_dim = 200\n",
    "depth = 12\n",
    "num_heads = 12\n",
    "mlp_ratio = 4.0\n",
    "qkv_bias = False\n",
    "qk_scale = None\n",
    "drop_rate = 0.0\n",
    "attn_drop_rate = 0.0\n",
    "drop_path_rate = 0.0\n",
    "norm_layer = nn.LayerNorm\n",
    "init_values = None\n",
    "tubelet_size = 2\n",
    "use_checkpoint = False\n",
    "use_learnable_pos_emb = False\n",
    "num_frames = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9106646",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Optical_Flow  = PretrainVisionTransformerEncoder(\n",
    "    img_size=img_size, patch_size=patch_size, in_chans=in_chans, num_classes=num_classes,\n",
    "    embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n",
    "    qk_scale=qk_scale, drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, drop_path_rate=drop_path_rate,\n",
    "    norm_layer=norm_layer, init_values=init_values, tubelet_size=tubelet_size, use_checkpoint=use_checkpoint,\n",
    "    use_learnable_pos_emb=use_learnable_pos_emb, num_frames=num_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "561e262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Lstm = LSTMModel(input_size, hidden_size, num_layers, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f860fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, num_heads):\n",
    "        super(Attention, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=feature_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.layer_norm = nn.LayerNorm(feature_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.multihead_attn(x, x, x)\n",
    "        attn_output = self.layer_norm(attn_output + x)\n",
    "        return attn_output.mean(dim=1)  # Aggregate across the sequence dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "571c6677",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModelWithAttention(nn.Module):\n",
    "    def __init__(self, model1, model2, feature_dim=512, num_heads=8, num_classes=11):\n",
    "        super(CombinedModelWithAttention, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.attention = Attention(712 , num_heads)  # feature_dim * 2 because of concatenation\n",
    "        self.fc_combined = nn.Linear( 712, num_classes)  # Assuming the output of each model is a 512-dimensional feature vector\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        features1 = self.model1(x1)  # [batch_size, 512]\n",
    "        features2 = self.model2(x2)  # [batch_size, 512]\n",
    "        combined_features = torch.cat((features1, features2), dim=1) # [batch_size, 1024]\n",
    "        combined_features = combined_features.unsqueeze(1)  # [batch_size, 1, 1024]\n",
    "        attended_features = self.attention(combined_features)  # [batch_size, 1024]\n",
    "        out = self.fc_combined(attended_features)  # [batch_size, num_classes]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a5dfce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "370b5a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model_with_attention = CombinedModelWithAttention(model_Optical_Flow, model_Lstm).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776c8604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10], Train Loss: 1.0630, Validation Loss: 0.9333, Validation Accuracy: 66.36%\n",
      "Epoch [1/10], Train Loss: 0.7883, Validation Loss: 1.0091, Validation Accuracy: 65.75%\n",
      "Epoch [2/10], Train Loss: 0.6932, Validation Loss: 0.8842, Validation Accuracy: 70.81%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(combined_model_with_attention.parameters(), lr=0.0001)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    combined_model_with_attention.train()\n",
    "    running_loss = 0.0\n",
    "    for videos1, videos2, labels in train_loader:\n",
    "        videos1, videos2, labels = videos1.to(device), videos2.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = combined_model_with_attention(videos1, videos2)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    combined_model_with_attention.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for videos1, videos2, labels in val_loader:\n",
    "            videos1, videos2, labels = videos1.to(device), videos2.to(device), labels.to(device)\n",
    "            outputs = combined_model_with_attention(videos1, videos2)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    \n",
    "    torch.save(combined_model_with_attention.state_dict(), 'LSTM_Optical_Flow_ViVit_Weights_Action_Classification/' + f'combined_model_epoch_ROMP_LSTM_{epoch}.pth')\n",
    "\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}], Train Loss: {train_loss:.4f}, \"\n",
    "          f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa71defa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e2e446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
